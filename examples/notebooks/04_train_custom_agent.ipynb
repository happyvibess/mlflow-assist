{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Custom Agents with MLFlow-Assist ðŸ¤–\n",
    "\n",
    "This notebook shows how to train custom AI agents using MLFlow-Assist. We'll create a simple reinforcement learning agent that learns to solve the CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from mlflow_assist.agents import RLAgent, RLConfig, AgentTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the Environment and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class RLDataset(Dataset):\n",
    "    def __init__(self, transitions):\n",
    "        self.states = [t[0] for t in transitions]\n",
    "        self.actions = [t[1] for t in transitions]\n",
    "        self.rewards = [t[2] for t in transitions]\n",
    "        self.next_states = [t[3] for t in transitions]\n",
    "        self.dones = [t[4] for t in transitions]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'states': self.states[idx],\n",
    "            'actions': self.actions[idx],\n",
    "            'rewards': self.rewards[idx],\n",
    "            'next_states': self.next_states[idx],\n",
    "            'dones': self.dones[idx]\n",
    "        }\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure and Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configure the agent\n",
    "config = RLConfig(\n",
    "    state_size=4,  # CartPole has 4 state dimensions\n",
    "    action_size=2,  # CartPole has 2 actions\n",
    "    hidden_size=128,\n",
    "    learning_rate=3e-4\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "agent = RLAgent(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collect Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def collect_episode(env, agent, max_steps=500):\n",
    "    state = env.reset()\n",
    "    transitions = []\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        action = agent.predict(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        transitions.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return transitions\n",
    "\n",
    "# Collect initial training data\n",
    "episodes = [collect_episode(env, agent) for _ in range(10)]\n",
    "train_dataset = RLDataset([t for e in episodes for t in e])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create trainer\n",
    "trainer = AgentTrainer(\n",
    "    agent=agent,\n",
    "    train_loader=train_loader,\n",
    "    experiment_name=\"cartpole-training\"\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "trainer.train(\n",
    "    num_epochs=50,\n",
    "    eval_every=5,\n",
    "    save_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_agent(env, agent, episodes=10):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.predict(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "        total_rewards.append(episode_reward)\n",
    "        \n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "# Test the agent\n",
    "mean_reward = evaluate_agent(env, agent)\n",
    "print(f\"Average reward over 10 episodes: {mean_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now you can:\n",
    "1. Try different environments\n",
    "2. Customize the agent architecture\n",
    "3. Experiment with hyperparameters\n",
    "4. Add more advanced features\n",
    "\n",
    "For more examples, check out the other notebooks in the `examples` directory!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

